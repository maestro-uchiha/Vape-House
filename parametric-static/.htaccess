# /htdocs/.htaccess  (Uberspace docroot: /var/www/virtual/maestro/html)
# Safer defaults
Options -Indexes -MultiViews

# Use our custom 404 (file must be at /404.html under docroot)
ErrorDocument 404 /404.html

# Caching (optional)
<IfModule mod_headers.c>
  <FilesMatch "\.html?$">
    Header set Cache-Control "no-cache"
  </FilesMatch>
  <FilesMatch "\.(css|js|png|jpe?g|gif|svg|ico|webp|woff2?)$">
    Header set Cache-Control "public, max-age=31536000, immutable"
  </FilesMatch>
</IfModule>

<IfModule mod_rewrite.c>
RewriteEngine On

# --- Always allow the 404 itself (avoid accidental blocking) ---
RewriteRule ^404\.html$ - [L]

# --- Your bot rules (unchanged logic, just grouped) ---
# Block empty UA OR "Mozilla" imposters that don't also match a real browser/bot UA
RewriteCond %{HTTP_USER_AGENT} ^\s*$ [OR]
RewriteCond %{HTTP_USER_AGENT} ^Mozilla [NC]
RewriteCond %{HTTP_USER_AGENT} !chrome [NC]
RewriteCond %{HTTP_USER_AGENT} !safari [NC]
RewriteCond %{HTTP_USER_AGENT} !firefox [NC]
RewriteCond %{HTTP_USER_AGENT} !edge [NC]
RewriteCond %{HTTP_USER_AGENT} !brave [NC]
RewriteCond %{HTTP_USER_AGENT} !opera [NC]
RewriteCond %{HTTP_USER_AGENT} !vivaldi [NC]
RewriteCond %{HTTP_USER_AGENT} !samsungbrowser [NC]
RewriteCond %{HTTP_USER_AGENT} !facebookexternalhit [NC]
RewriteCond %{HTTP_USER_AGENT} !linkedinbot [NC]
RewriteCond %{HTTP_USER_AGENT} !twitterbot [NC]
RewriteCond %{HTTP_USER_AGENT} !instagram [NC]
RewriteCond %{HTTP_USER_AGENT} !pinterest [NC]
RewriteCond %{HTTP_USER_AGENT} !tiktok [NC]
RewriteCond %{HTTP_USER_AGENT} !whatsapp [NC]
RewriteCond %{HTTP_USER_AGENT} !googlebot [NC]
RewriteCond %{HTTP_USER_AGENT} !bingbot [NC]
RewriteCond %{HTTP_USER_AGENT} !slurp [NC]
RewriteCond %{HTTP_USER_AGENT} !duckduckbot [NC]
RewriteCond %{HTTP_USER_AGENT} !yandexbot [NC]
RewriteCond %{HTTP_USER_AGENT} !chatgpt-user [NC]
RewriteCond %{HTTP_USER_AGENT} !gptbot [NC]
RewriteCond %{HTTP_USER_AGENT} !perplexitybot [NC]
RewriteCond %{HTTP_USER_AGENT} !youbot [NC]
RewriteCond %{HTTP_USER_AGENT} !google-extended [NC]
RewriteCond %{HTTP_USER_AGENT} !anthropicbot [NC]
RewriteCond %{HTTP_USER_AGENT} !neevabot [NC]
RewriteCond %{HTTP_USER_AGENT} !amazonbot [NC]
RewriteRule .* - [F,L]

# Block unknown bots from robots/sitemap access (but let known ones pass)
RewriteCond %{REQUEST_URI} (robots\.txt|sitemap.*\.xml)$ [NC]
RewriteCond %{HTTP_USER_AGENT} !(googlebot|bingbot|slurp|duckduckbot|yandexbot|ahrefsbot|semrushbot|mj12bot|rogerbot|dotbot|ubersuggest|chatgpt-user|gptbot|perplexitybot|youbot|google-extended|anthropicbot|neevabot|amazonbot) [NC]
RewriteRule ^.*$ - [F,L]

# --- Fallback 404 for anything that doesn't exist ---
# (Only triggers when the request is not a real file/dir and no rule above matched)
RewriteCond %{REQUEST_FILENAME} !-f
RewriteCond %{REQUEST_FILENAME} !-d
RewriteRule ^ /404.html [L,R=404]

</IfModule>
